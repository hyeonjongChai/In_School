{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa4f4c7",
   "metadata": {},
   "source": [
    "### 1. Pytorch로 Softmax Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e58bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ]) \n",
    "y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ccbd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost: 1.098612\n",
      "epoch: 100, cost: 0.279874\n",
      "epoch: 200, cost: 0.162209\n",
      "epoch: 300, cost: 0.105263\n",
      "epoch: 400, cost: 0.074083\n",
      "epoch: 500, cost: 0.055093\n",
      "epoch: 600, cost: 0.042634\n",
      "epoch: 700, cost: 0.034003\n",
      "epoch: 800, cost: 0.027768\n",
      "epoch: 900, cost: 0.023111\n",
      "epoch: 1000, cost: 0.019535\n",
      "epoch: 1100, cost: 0.016727\n",
      "epoch: 1200, cost: 0.014479\n",
      "epoch: 1300, cost: 0.012650\n",
      "epoch: 1400, cost: 0.011140\n",
      "epoch: 1500, cost: 0.009879\n",
      "epoch: 1600, cost: 0.008813\n",
      "epoch: 1700, cost: 0.007905\n",
      "epoch: 1800, cost: 0.007124\n",
      "epoch: 1900, cost: 0.006447\n",
      "epoch: 2000, cost: 0.005856\n",
      "epoch: 2100, cost: 0.005338\n",
      "epoch: 2200, cost: 0.004881\n",
      "epoch: 2300, cost: 0.004475\n",
      "epoch: 2400, cost: 0.004113\n",
      "epoch: 2500, cost: 0.003789\n",
      "epoch: 2600, cost: 0.003499\n",
      "epoch: 2700, cost: 0.003236\n",
      "epoch: 2800, cost: 0.002999\n",
      "epoch: 2900, cost: 0.002784\n",
      "epoch: 3000, cost: 0.002588\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(4, 3, requires_grad = True)\n",
    "b = torch.zeros(1, 3, requires_grad = True)\n",
    "\n",
    "optimizer = torch.optim.Adam([W, b], lr = 0.1)\n",
    "\n",
    "for epoch in range(3001):\n",
    "    hypothesis = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
    "    # hypothesis = (torch.mm(x_train, W)+b).softmax(dim=1)\n",
    "    cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim = 1))\n",
    "    # cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e63dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 5.5165e-19, 7.0149e-38],\n",
      "        [1.4799e-02, 7.4294e-01, 2.4226e-01],\n",
      "        [1.2256e-33, 9.0835e-12, 1.0000e+00]])\n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "W.requires_grad_(False) \n",
    "b.requires_grad_(False)\n",
    "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) \n",
    "test_all = (torch.mm(x_test, W)+b).softmax(dim=1) \n",
    "print(test_all) \n",
    "print(torch.argmax(test_all, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118075fb",
   "metadata": {},
   "source": [
    "### 2. 조금 더 깔끔하게 Softmax\n",
    "    - 마음에 안드는 부분 1. [1,0,0], [0,1,0], [0,0,1] 대신 0, 1, 2를 쓰면 안되나?\n",
    "    - 마음에 안드는 부분 2. 이렇게 복잡한 함수를 항상 직접 구현해야하나? 어차피 softmax, cross entropy인데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5e66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ]) \n",
    "y_train = torch.LongTensor([2,2,2,1,1,1,0,0]) # longtensor? onehot-encoding 된 것을 0,1,2로?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08c590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost: 1.831898\n",
      "epoch: 100, cost: 0.061686\n",
      "epoch: 200, cost: 0.032587\n",
      "epoch: 300, cost: 0.020433\n",
      "epoch: 400, cost: 0.014072\n",
      "epoch: 500, cost: 0.010327\n",
      "epoch: 600, cost: 0.007929\n",
      "epoch: 700, cost: 0.006296\n",
      "epoch: 800, cost: 0.005129\n",
      "epoch: 900, cost: 0.004264\n",
      "epoch: 1000, cost: 0.003603\n",
      "epoch: 1100, cost: 0.003085\n",
      "epoch: 1200, cost: 0.002672\n",
      "epoch: 1300, cost: 0.002335\n",
      "epoch: 1400, cost: 0.002058\n",
      "epoch: 1500, cost: 0.001826\n",
      "epoch: 1600, cost: 0.001631\n",
      "epoch: 1700, cost: 0.001464\n",
      "epoch: 1800, cost: 0.001320\n",
      "epoch: 1900, cost: 0.001195\n",
      "epoch: 2000, cost: 0.001086\n",
      "epoch: 2100, cost: 0.000991\n",
      "epoch: 2200, cost: 0.000907\n",
      "epoch: 2300, cost: 0.000832\n",
      "epoch: 2400, cost: 0.000765\n",
      "epoch: 2500, cost: 0.000705\n",
      "epoch: 2600, cost: 0.000651\n",
      "epoch: 2700, cost: 0.000603\n",
      "epoch: 2800, cost: 0.000559\n",
      "epoch: 2900, cost: 0.000519\n",
      "epoch: 3000, cost: 0.000483\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(4,3) \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1)\n",
    "\n",
    "for epoch in range(3001):\n",
    "    z = model(x_train)\n",
    "    cost = F.cross_entropy(z, y_train) # 주의! F.cross_entropy 는 softmax와 cross entropy를 합친 것.\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2995c",
   "metadata": {},
   "source": [
    "### 3. Softmax Regression in Sklearn\n",
    "\n",
    "sklearn에는 LogisticRegression에 Softmax regression이 함께 구현됨  \n",
    "⇒ y에 두 종류 이상의 값이 있을 경우 softmax regression 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21ac411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], \n",
    "                    [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
    "y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])  # y에 0, 1, 2 등 둘 이상의 class가 존재 => softmax regression \n",
    "\n",
    "logistic = LogisticRegression() # 모델 생성 \n",
    "logistic.fit(x_train, y_train) # 학습\n",
    "pred = logistic.predict([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) # test case (값 예측) \n",
    "print(pred) # 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
